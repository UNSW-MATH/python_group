{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language analysis using Python and Spacy\n",
    "\n",
    "We will use the Python module Spacy to analize the [Malcolm Fraser Electorate Radio Talks](https://archives.unimelb.edu.au/explore/collections/malcolmfraser/explore/radiotalks) archives. On the way we will learn a couple of other tricks to make our lives easier.\n",
    "\n",
    "\n",
    "```\n",
    "wget https://archives.unimelb.edu.au/__data/assets/text_file/0006/1717746/UMA_Fraser_Radio_Talks.zip\n",
    "pip3 install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "That gets us ready to do the linguistic work.\n",
    "\n",
    "But before that, we need to read the data in and clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we see the files? Yes - here are the titles. \n",
    "import os\n",
    "files = os.listdir('UMA_Fraser_Radio_Talks')\n",
    "print(files[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are txt files - the file type that traditionally contains text. Great, let's look inside them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we read the files as text? Yes... No.\n",
    "f = open(os.path.join('UMA_Fraser_Radio_Talks', files[0]), \"r\")\n",
    "text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnicodeDecode Errors - character sets, unicode and automation\n",
    "\n",
    "UnicodeDecode errors will be frequent in Python version 3 - they have changed how they represent human language within the Python language. While it's annoying - and slightly more work - the solution we implement below is relatively quick and solves the problem *regardless of the language of the documents we are researching*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we read the files as binary text files? Yes!\n",
    "f = open(os.path.join('UMA_Fraser_Radio_Talks', files[0]), 'rb')\n",
    "text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks better. What was that **b** we added to the open path? That told python to open the file to (r)ead - but to read the file as a byte stream instead of as text<sup>1</sup>. That's handy, but why and is it useful? \n",
    "\n",
    "<sup>1</sup> with the default ASCII encoding. This is important later.\n",
    "\n",
    "Well, we can now read the file - and we can see the section near the bottom of the file that caused the issue. That second box shows another example - although in this case it's valid ASCII.\n",
    "\n",
    "\n",
    "![Hex Chars](imgs/non-text-characters_small.png \"Hex Chars\")\n",
    "\n",
    "\n",
    "What we are seeing here is a data quality issue. This is very common, especially with text that has been scanned from PDFs. The image to text transfer will make a best guess, and in this case it's guessed an unusual character. If you take a look at the [original pdf](https://digitised-collections.unimelb.edu.au/bitstream/handle/11343/40335/312821_2007-0023-0372.pdf), you can see that this is because Mr Cain had made hand written notes on the bottom of his talk. \n",
    "\n",
    "The character combination **\\x** is a restriction within [ASCII](https://en.wikipedia.org/wiki/ASCII) text files - it is a leading indicator used to encode characters as bytes. For instance \\x20 is the space character, \\x3a is the colon character (:) and \\x41 is capital A. \n",
    "\n",
    "We wont go into character encoding now, but we will show into how to solve the problem. The problem starts with **\\x84** being an invalid ASCII code - **\\x7F** (the \"Delete\" character) is as high as ASCII goes. If you really want to investigate further, we recommend [this primer on Unicode as a great start](https://wisdom.engineering/awesome-unicode/).\n",
    "\n",
    "Why do we want to solve this - why not just import the files as \"bytestreams\"? The primary reason is that text manipulation in python is powerful and easy to use - but it only works on text, not bytestreams. \n",
    "\n",
    "Thankfully, those that came before us have solved the problem of guessing the encoding and have written a Python modue called Chardet (\"character detect\") to solve it. How did I know to use this software? I did a search for \"python determine character set\".\n",
    "\n",
    "### Character Set Detection\n",
    "\n",
    "Let's install chardet and use that to get an idea for the probable character encoding. This code is copy and pasted from [the Chardet documentation](https://chardet.readthedocs.io/en/latest/usage.html#example-detecting-encodings-of-multiple-files) and slightly altered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "detector = UniversalDetector()\n",
    "\n",
    "for f in files[20:25]:\n",
    "    filename=os.path.join('UMA_Fraser_Radio_Talks',f)\n",
    "    detector.reset()\n",
    "    for line in open(filename, 'rb'):\n",
    "        detector.feed(line)\n",
    "        if detector.done: break\n",
    "    detector.close()\n",
    "    print(f, \": \", detector.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally all text files for what are known English Language texts would just be in English, but as you can see the data isn't as clean as we would like. In fact, there are at least three detected character encodings - **Windows-1252**, **ASCII** and **ISO-8859-1**. \n",
    "\n",
    "We will need to deal with this on a file by file basis - but thankfully we don't actually need to know what each of those encodings represents. \n",
    "\n",
    "The reality is that - like with our first file - the problem is dirty data problem. A single misreaded character by the Optical Character Recognition software and everything is a mess.\n",
    "\n",
    "Let's turn the above into a function that just returns the encoding so we can use it repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_charset(filename):\n",
    "\n",
    "    import chardet\n",
    "    from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "    detector = UniversalDetector()\n",
    "    detector.reset()\n",
    "    for line in open(filename, 'rb'):\n",
    "        detector.feed(line)\n",
    "        if detector.done: break\n",
    "    detector.close()\n",
    "\n",
    "    return detector.result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('UMA_Fraser_Radio_Talks', files[0])\n",
    "encoding = get_charset(filename)\n",
    "f = open(filename, 'r', encoding=encoding)\n",
    "text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Data\n",
    "\n",
    "Now that looks much better. \n",
    "\n",
    "Let's keep cleaning it up. Looks like we can remove the metadata from the top. We could put it into a database or a dictionary for later use if we want. For the moment, let's just split it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text.split(\"<!--end metadata-->\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Let's see if we can't start making some sense of what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into lines, add '*' to the start of each line\n",
    "# \\n is a newline character\n",
    "for line in data[0].split('\\n'):\n",
    "    print('*', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip empty lines and any line that starts with '<'\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line.startswith('<'):\n",
    "        continue\n",
    "    print('*', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the metadata items on ':' so that we can interrogate each one\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line[0] == '<':\n",
    "        continue\n",
    "    element = line.split(':')\n",
    "    print('*', element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually, only split on the first colon\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line[0] == '<':\n",
    "        continue\n",
    "    element = line.split(':', 1)\n",
    "    print('*', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Let's put it into a dictionary so we can use it later. \n",
    "\n",
    "The point of dictionaries is to store a key (the word) and a value (the count). When you ask for the key, you get its value.\n",
    "\n",
    "Notice that you use curly braces for dictionaries, but square brackets for lists.\n",
    "\n",
    "Dictionaries are a great way to work with the metadata in our corpus. Let's build a dictionary called metadata:\n",
    "\n",
    "Your first line will look like this:\n",
    "\n",
    "  metadata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line[0] == '<':\n",
    "        continue\n",
    "    element = line.split(':', 1)\n",
    "    metadata[element[0]] = element[-1]\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn that into a function as well - we will be coming back to it for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata(text):\n",
    "    metadata = {}\n",
    "    for line in text.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        if line[0] == '<':\n",
    "            continue\n",
    "        element = line.split(':', 1)\n",
    "        metadata[element[0]] = element[-1].strip(' ')\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = parse_metadata(data[0])\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata collection\n",
    "Now that we have all the tools we need to collect each file's metadata, let's do put it into a data structure so we can do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraser_talks_metadata = {}\n",
    "\n",
    "for file in os.listdir('UMA_Fraser_Radio_Talks'):\n",
    "    # If anything goes wrong, we will know which files to look at.\n",
    "    try:\n",
    "        filename = os.path.join('UMA_Fraser_Radio_Talks', file)\n",
    "        encoding = get_charset(filename)\n",
    "        text = open(filename, 'r', encoding=encoding).read()\n",
    "    except:\n",
    "        print(\"file is\", filename, \" and it's chardet data is\", get_charset(filename))\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    #split text of file on 'end metadata'\n",
    "    data = text.split(\"<!--end metadata-->\")\n",
    "    \n",
    "    #parse metadata using previously defined function \"parse_metadata\"\n",
    "    metadata = parse_metadata(data[0])\n",
    "    talk_data = data[1]\n",
    "      \n",
    "    fraser_talks_metadata[file]={'metadata':metadata, 'talk_data':talk_data}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every error will be fixed. \n",
    "\n",
    "Something is wrong with that file. We will need to open it up to take a look. When we [open the file in jupyter notebook](UMA_Fraser_Radio_Talks/UDS2013680-152-full.txt) we can see at the very bottom two odd looking characters.\n",
    "\n",
    "![](imgs/weird_characters.png \"asda\") \n",
    "             \n",
    "What we have found is a document with conflicted information about what character encoding it is - those two characters are usual. On inspection, they are 0xC (the Form Feed character) and 0xDC (unknown). \n",
    "\n",
    "Since it's only one file and the process for discovery is nerdy and tedious, we will re-run the above with what professionals call [a *hack*](workingprocess.ipynb). I will put in a one time exception for this single file. You will not pass a PhD with hacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraser_talks_metadata = {}\n",
    "\n",
    "for file in os.listdir('UMA_Fraser_Radio_Talks'):\n",
    "    # If anything goes wrong, we will know which files to look at.\n",
    "    try:\n",
    "        filename = os.path.join('UMA_Fraser_Radio_Talks', file)\n",
    "        encoding = get_charset(filename)\n",
    "        text = open(filename, 'r', encoding=encoding).read()\n",
    "    except:\n",
    "        # Special case: open the file in as binary, only read the first 2650 bytes\n",
    "        # once they are read, decode the binary as ascii\n",
    "        file_handle = open(filename, 'rb')\n",
    "        text = file_handle.read(2650).decode('ascii')\n",
    "            \n",
    "    #split text of file on 'end metadata'\n",
    "    data = text.split(\"<!--end metadata-->\")\n",
    "    \n",
    "    #parse metadata using previously defined function \"parse_metadata\"\n",
    "    metadata = parse_metadata(data[0])\n",
    "    talk_data = data[1]\n",
    "      \n",
    "    fraser_talks_metadata[file]={'metadata':metadata, 'talk_data':talk_data}\n",
    "\n",
    "fraser_talks_metadata['UDS2013680-152-full.txt']['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#fraser_talks_metadata.keys()\n",
    "\n",
    "dates = []\n",
    "\n",
    "for file_id in fraser_talks_metadata:\n",
    "    date = fraser_talks_metadata[file_id]['metadata']['Date']\n",
    "    if date.startswith('c'):\n",
    "    # date format cyyyy\n",
    "        year = date[1:]\n",
    "    elif len(date) == 10:\n",
    "    # date format dd/mm/yyyy\n",
    "        year = date[6:]\n",
    "    elif len(date) == 9:\n",
    "    # date format d/mm/yyyy\n",
    "        year = date[5:]\n",
    "    if len(year) == 5:\n",
    "    # pesky space in 1969\n",
    "        year = year.lstrip()\n",
    "    # Let's add the year to the metadata for later\n",
    "    fraser_talks_metadata[file_id]['metadata'] = {'year':year}\n",
    "    dates.append(year)\n",
    "        \n",
    "Counter(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we order those years? Probably. At the moment they are strings, but we will want to order them as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_summary = OrderedDict(sorted(Counter(dates).items(), key=lambda t: t))\n",
    "data_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO describe splicing arrays and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO plot that distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data_summary = Counter(dates)\n",
    "\n",
    "# we make a bar graph, the y axis will be the values in the data_summary\n",
    "plt.bar(range(len(data_summary)), list(data_summary.values()),align='center')\n",
    "\n",
    "# we add in the x axis details - and turn the years around so they fit\n",
    "plt.xticks(range(len(data_summary)), list(data_summary.keys()),rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. That looks great. Right away we can see that something momentous is happening in 1966 and 1967 - the lowest and highest number of talks respectively. If we look at his Wikipedia page, we can see that '66 is the year that he becomes a Government minister for the first time - and it's the Ministry for the Army as we enter the Vietnam War. \n",
    "\n",
    "Those records will be interesting to look at more closely later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic analysis\n",
    "\n",
    "There are many things that can now be done. What theat might be depends on how much linguistic gymnastics you want to do. \n",
    "\n",
    "I'm going to start with some simple \"shape of the data\" anaylses. Let's see the length of each talk, averages across the years, number of sentences - easy stuff to do with what we've learnt. \n",
    "\n",
    "Then we will utilise some of spaCy's real power to count how many of each of the [parts of speech](https://spacy.io/api/annotation#pos-tagging) exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "fraser_talks_data_shape = {}\n",
    "\n",
    "for file_id in fraser_talks_metadata:\n",
    "    year = fraser_talks_metadata[file_id]['metadata']['year']\n",
    "    doc = fraser_talks_metadata[file_id]['talk_data']\n",
    "    nlp_doc = nlp(doc)\n",
    "    \n",
    "    word_count = len(nlp_doc)\n",
    "    \n",
    "    sentences = list(nlp_doc.sents)\n",
    "    sentence_count = len(sentences)\n",
    "      \n",
    "    fraser_talks_data_shape[file_id] = {'word_count':word_count,'sentences':sentence_count, 'year':year}\n",
    "\n",
    "    \n",
    "    #Now let's count the Parts of Speech\n",
    "    # Returns integers that map to parts of speech\n",
    "    counts_dict = nlp_doc.count_by(spacy.attrs.IDS['POS'])\n",
    "\n",
    "    # Print the human readable part of speech tags\n",
    "    for pos, count in counts_dict.items():\n",
    "        human_readable_tag = nlp_doc.vocab[pos].text\n",
    "        fraser_talks_data_shape[file_id][human_readable_tag] = count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for talk in list(fraser_talks_data_shape)[0:3]:\n",
    "    print(talk,\":\",fraser_talks_data_shape[talk],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get some common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "decade_stats = {}\n",
    "\n",
    "for file_id in list(fraser_talks_metadata)[0:3]:\n",
    "    year = fraser_talks_metadata[file_id]['metadata']['year']\n",
    "    doc = fraser_talks_metadata[file_id]['talk_data']\n",
    "    nlp_doc = nlp(doc)\n",
    "\n",
    "    # all tokens that arent stop words, punctuation or white space.\n",
    "    # spaCy prides itself on it's token list being an exact replica of the source.\n",
    "    # We don't want \"line end\" to be the most common word\n",
    "    words = [token.text for token in nlp_doc if token.is_stop != True and token.is_punct != True and token.is_space != True]\n",
    "\n",
    "    # noun tokens that arent stop words or punctuations\n",
    "    nouns = [token.text for token in nlp_doc if token.is_stop != True and token.is_punct != True and token.is_space != True and token.pos_ == \"NOUN\"]\n",
    "\n",
    "    # five most common tokens\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common(5)\n",
    "\n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = noun_freq.most_common(5)\n",
    "        \n",
    "    if year in decade_stats:\n",
    "        decade_stats[year]['words'] + words\n",
    "        decade_stats[year]['nouns'] + nouns\n",
    "    else:\n",
    "        decade_stats[year] = {'words':words, 'nouns':nouns}\n",
    "\n",
    "    print(file_id, '\\n','common words', common_words,'\\n' 'common nouns', common_nouns, '\\n')\n",
    "        \n",
    "for year in decade_stats.keys():\n",
    "    # five most common tokens\n",
    "    word_freq = Counter(decade_stats[year]['words'])\n",
    "    common_words = word_freq.most_common(5)\n",
    "\n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(decade_stats[year]['nouns'])\n",
    "    common_nouns = noun_freq.most_common(5)\n",
    "\n",
    "    print(year, common_words, common_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "decade_stats = {}\n",
    "\n",
    "for file_id in list(fraser_talks_metadata):\n",
    "    year = fraser_talks_metadata[file_id]['metadata']['year']\n",
    "    doc = fraser_talks_metadata[file_id]['talk_data']\n",
    "    nlp_doc = nlp(doc.lower())\n",
    "\n",
    "    # all tokens that arent stop words, punctuation or white space.\n",
    "    # spaCy prides itself on it's token list being an exact replica of the source.\n",
    "    # We don't want \"line end\" to be the most common word\n",
    "    words = [token.text for token in nlp_doc if token.is_stop != True and token.is_punct != True and token.is_space != True]\n",
    "\n",
    "    # noun tokens that arent stop words or punctuations\n",
    "    nouns = [token.text for token in nlp_doc if token.is_stop != True and token.is_punct != True and token.is_space != True and token.pos_ == \"NOUN\"]\n",
    "\n",
    "    # five most common tokens\n",
    "    # and lets lowercase everything\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common(5)\n",
    "\n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = noun_freq.most_common(5)\n",
    "    \n",
    "    fraser_talks_data_shape[file_id]['common words'] = common_words\n",
    "    fraser_talks_data_shape[file_id]['common nouns'] = common_nouns\n",
    "    \n",
    "    decade = year[2]\n",
    "    \n",
    "    # per year information\n",
    "    if year in decade_stats:\n",
    "        decade_stats[year]['words'] + words\n",
    "        decade_stats[year]['nouns'] + nouns\n",
    "    else:\n",
    "        decade_stats[year] = {'words':words, 'nouns':nouns}\n",
    "\n",
    "    # per decade information1957 [('Henty', 10), ('Register', 5), ('James', 4), ('Portland', 3), ('shipping', 3)] [('shipping', 3), ('years', 2), ('entry', 2), ('movements', 2), ('family', 2)]\n",
    "    if decade in decade_stats:\n",
    "        decade_stats[decade]['words'] + words\n",
    "        decade_stats[decade]['nouns'] + nouns\n",
    "    else:\n",
    "        decade_stats[decade] = {'words':words, 'nouns':nouns}\n",
    "\n",
    "\n",
    "for year in data_summary.keys():\n",
    "    # five most common tokens\n",
    "    word_freq = Counter(decade_stats[year]['words'])\n",
    "    common_words = word_freq.most_common(5)\n",
    "    decade_stats[year]['common_words'] = common_words\n",
    "    \n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(decade_stats[year]['nouns'])\n",
    "    common_nouns = noun_freq.most_common(5)\n",
    "    decade_stats[year]['common_nouns'] = common_nouns\n",
    "    \n",
    "    #print(year, common_words, common_nouns)\n",
    "    print(year, common_words[0], common_nouns[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for decade in '5','6','7','8':\n",
    "    # five most common tokens\n",
    "    word_freq = Counter(decade_stats[decade]['words'])\n",
    "    common_words = word_freq.most_common(5)\n",
    "    decade_stats[decade]['common_words'] = common_words\n",
    "    \n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(decade_stats[decade]['nouns'])\n",
    "    common_nouns = noun_freq.most_common(5)\n",
    "    decade_stats[decade]['common_nouns'] = common_nouns\n",
    "    \n",
    "    #print(year, common_words, common_nouns)\n",
    "    print(decade, common_words, common_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump([fraser_talks_data_shape,fraser_talks_metadata,data_summary,decade_stats], fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lemmitization is the linguistic process of grouping together similar words by decomposing them to their common \n",
    "# form\n",
    "\n",
    "\"\"\" \n",
    "From the [Stanford NLP book](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related \n",
    "forms of a word to a common base form. For instance:\n",
    "\n",
    "    am, are, is -> be\n",
    "    car, cars, car's, cars' -> car \n",
    "\n",
    "The result of this mapping of text will be something like:\n",
    "\n",
    "    the boy's cars are different colors ->\n",
    "    the boy car be differ color \n",
    "\"\"\"\n",
    "    \n",
    "for file_id in list(fraser_talks_metadata):\n",
    "    year = fraser_talks_metadata[file_id]['metadata']['year']\n",
    "    doc = fraser_talks_metadata[file_id]['talk_data']\n",
    "    nlp_doc = nlp(doc.lower())\n",
    "\n",
    "    # all tokens that arent stop words, punctuation or white space.\n",
    "    # spaCy prides itself on it's token list being an exact replica of the source.\n",
    "    # We don't want \"line end\" to be the most common word\n",
    "    lemmas = [token.lemma_ for token in nlp_doc if token.is_stop != True and token.is_punct != True and token.is_space != True]\n",
    "\n",
    "    # five most common tokens\n",
    "    # and lets lowercase everything\n",
    "    lemmas_freq = Counter(lemmas)\n",
    "    common_lemmas = lemmas_freq.most_common(5)\n",
    "\n",
    "    fraser_talks_data_shape[file_id]['common lemmas'] = common_lemmas\n",
    "       \n",
    "    decade = year[2]\n",
    "    \n",
    "    # per year information\n",
    "    decade_stats[year] = {'lemmas':lemmas}\n",
    "    \n",
    "    # per decade information\n",
    "    decade_stats[decade] = {'lemmas':lemmas}\n",
    "\n",
    "\n",
    "for year in data_summary.keys():\n",
    "    # five most common tokens\n",
    "    lemmas_freq = Counter(decade_stats[year]['lemmas'])\n",
    "    common_lemmas = word_freq.most_common(5)\n",
    "    decade_stats[year]['common_lemmas'] = common_lemmas\n",
    "    \n",
    "    #print(year, common_words, common_nouns)\n",
    "    print(year, common_lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
